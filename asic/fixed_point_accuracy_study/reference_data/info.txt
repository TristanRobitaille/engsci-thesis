VISION TRANSFORMER MODEL TRAINING SUMMARY
Git hash: d920ab83d91666ab1f22bed2cb08c52e83a288f6
Time to complete: 1687.25s
Model: "vision_transformer"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 patch_projection_dense (Den  multiple                 4160      
 se)                                                             
                                                                 
 rescaling (Rescaling)       multiple                  0         
                                                                 
 Encoder_1 (Encoder)         multiple                  21088     
                                                                 
 mlp_head_layerNorm (LayerNo  multiple                 128       
 rmalization)                                                    
                                                                 
 mlp_head (Sequential)       (None, 32)                2080      
                                                                 
 mlp_head_softmax (Dense)    multiple                  165       
                                                                 
=================================================================
Total params: 31,589
Trainable params: 31,589
Non-trainable params: 0
_________________________________________________________________
Dataset: /home/tristanr/projects/def-xilinliu/tristanr/engsci-thesis/python_prototype/data/SS3_EDF_Tensorized_both_light_deep_combine-stg_30-0s_128Hz_15b_offset
Number of operations: {'adds': 3667730, 'subs': 11712, 'mults': 3707144, 'divs': 46174, 'acts': 8113, 'incrs': 3722212, 'exps': 488, 'sqrts': 184}
Save model: True
Save k-fold validation file: True
File path of model to load (model trained if empty string): None
Channel: EEG Cz-LER
k-fold validation set: 23
Validation set accuracy (tf): 0.8596
Validation set accuracy (tflite): 0.8596
Validation set accuracy (tflite (quant)): 0.8596
Validation set accuracy (tflite (full quant)): 0.5446
Validation set accuracy (tflite (16bx8b full quant)): 0
Training accuracy: [0.575, 0.6591, 0.6797, 0.6843, 0.6872, 0.6919, 0.6923, 0.6963, 0.6986, 0.7021, 0.706, 0.7169, 0.7295, 0.7374, 0.7366, 0.743, 0.7446, 0.7474, 0.7494, 0.7499, 0.7502, 0.7525, 0.7533, 0.7541, 0.7573, 0.7555, 0.7569, 0.7556, 0.7571, 0.7587, 0.7568, 0.7585, 0.759, 0.7593, 0.7607, 0.7603, 0.7617, 0.7635, 0.7619, 0.7644, 0.7625, 0.7631, 0.7644, 0.7645, 0.7649, 0.7682, 0.7669, 0.7663, 0.7671, 0.7661, 0.771, 0.7693, 0.77, 0.7709, 0.7706, 0.771, 0.7714, 0.7735, 0.7733, 0.7736, 0.7751, 0.7755, 0.778, 0.7766, 0.7801, 0.7781, 0.7817, 0.7815, 0.7817, 0.7818, 0.7853, 0.7837, 0.7882, 0.787, 0.7876, 0.7904, 0.7923, 0.7909, 0.7927, 0.7914, 0.7951, 0.7948, 0.7948, 0.7971, 0.7963, 0.7957, 0.7983, 0.797, 0.7993, 0.7987, 0.7981, 0.799, 0.7999, 0.8015, 0.8005, 0.8009, 0.8017, 0.802, 0.8008, 0.8025]
Validation accuracy (while training): [0.6824, 0.7485, 0.7597, 0.7536, 0.7526, 0.752, 0.7541, 0.7413, 0.7556, 0.7495, 0.7485, 0.7567, 0.7741, 0.7567, 0.7879, 0.7725, 0.7772, 0.7782, 0.7766, 0.7746, 0.7782, 0.7807, 0.7802, 0.7792, 0.7756, 0.7884, 0.7864, 0.7812, 0.7823, 0.7869, 0.7879, 0.792, 0.7853, 0.7843, 0.7884, 0.7833, 0.7889, 0.7879, 0.792, 0.7833, 0.79, 0.7705, 0.7946, 0.7797, 0.792, 0.7838, 0.7961, 0.793, 0.7976, 0.7982, 0.7946, 0.79, 0.8089, 0.7905, 0.8028, 0.8069, 0.8033, 0.8028, 0.8038, 0.8166, 0.811, 0.8115, 0.811, 0.8145, 0.8156, 0.8238, 0.8258, 0.8335, 0.8248, 0.8345, 0.8299, 0.8427, 0.8345, 0.8402, 0.8478, 0.8417, 0.8432, 0.8509, 0.8478, 0.8484, 0.8504, 0.8504, 0.8494, 0.8514, 0.8494, 0.8586, 0.8571, 0.8566, 0.8586, 0.8566, 0.8545, 0.8601, 0.8571, 0.8576, 0.8591, 0.8555, 0.8607, 0.8642, 0.8612, 0.8566]
Training loss: [1.0654, 0.8029, 0.7627, 0.7492, 0.7411, 0.7343, 0.7306, 0.7235, 0.7177, 0.7065, 0.69, 0.6711, 0.6425, 0.6282, 0.6186, 0.6099, 0.6023, 0.5996, 0.5919, 0.5918, 0.5879, 0.5838, 0.5821, 0.5803, 0.5783, 0.5768, 0.5745, 0.5732, 0.5719, 0.5704, 0.57, 0.5687, 0.566, 0.5658, 0.5644, 0.5636, 0.5619, 0.5604, 0.5599, 0.5586, 0.5591, 0.5591, 0.5567, 0.5563, 0.5549, 0.5539, 0.5522, 0.553, 0.552, 0.5512, 0.55, 0.5481, 0.5475, 0.5484, 0.5462, 0.5458, 0.5443, 0.5433, 0.541, 0.543, 0.5394, 0.538, 0.5371, 0.5364, 0.5342, 0.5315, 0.5318, 0.5297, 0.5277, 0.5261, 0.5245, 0.5225, 0.5203, 0.5191, 0.5173, 0.5139, 0.5122, 0.5131, 0.5118, 0.5097, 0.5074, 0.5081, 0.5053, 0.504, 0.5025, 0.504, 0.5, 0.5008, 0.4984, 0.4977, 0.4993, 0.4988, 0.4955, 0.494, 0.4947, 0.4944, 0.4925, 0.4927, 0.4927, 0.4912]
Validation loss (while training): [0.7866, 0.6132, 0.5953, 0.6248, 0.6033, 0.6002, 0.6088, 0.6159, 0.5869, 0.5889, 0.5814, 0.5868, 0.5502, 0.552, 0.5149, 0.5213, 0.5376, 0.5321, 0.5172, 0.5436, 0.5234, 0.5247, 0.5155, 0.5248, 0.5201, 0.5161, 0.5093, 0.5116, 0.5123, 0.5004, 0.5036, 0.5034, 0.5181, 0.5074, 0.4933, 0.5108, 0.5005, 0.5026, 0.4947, 0.5039, 0.4987, 0.5462, 0.4974, 0.5243, 0.5057, 0.5222, 0.4899, 0.4952, 0.4896, 0.4917, 0.4921, 0.4929, 0.4804, 0.5065, 0.4849, 0.4819, 0.4903, 0.4916, 0.4746, 0.4744, 0.475, 0.4776, 0.4812, 0.4704, 0.4801, 0.47, 0.4597, 0.445, 0.4593, 0.4336, 0.4357, 0.4297, 0.4433, 0.4382, 0.4197, 0.4217, 0.4253, 0.4062, 0.4044, 0.4043, 0.4065, 0.3959, 0.393, 0.4044, 0.3977, 0.3929, 0.3808, 0.3888, 0.3867, 0.3754, 0.3864, 0.3772, 0.3873, 0.3824, 0.3759, 0.3864, 0.373, 0.3767, 0.3689, 0.3768]
Number of epochs: 100

# of nights in validation: 2
Training set resampling: False
Training set resampling replacement: True
Training set shuffled: True
Training set resampler: ADASYN
Training set target count: [4600, 4600, 4600, 4600, 4600]
Use sleep stage history: False
Number of historical sleep stages: 0
Dataset split random seed: 42
Dataset metadata {
    "clip_length_s": 30.0,
    "data_type": "<dtype: 'uint16'>",
    "dataset_filesize_GB": 2.55,
    "fields": [
        "EEG C4-LER",
        "EEG Fp1-LER",
        "EEG Cz-LER",
        "EEG Fz-LER",
        "EEG F4-LER",
        "sleep_stage",
        "pseudo_random",
        "new_night_marker"
    ],
    "historical_lookback_length": 0,
    "num_files_used": 62,
    "num_stages": 4,
    "one_hot_encoding": false,
    "sampling_freq_Hz": 128,
    "signal_processing_operations": [
        "15b_offset"
    ],
    "single_channel": false,
    "sleep_stages_cnt": [
        0,
        7653,
        34641,
        10581,
        6442
    ],
    "sleep_stages_map_name": "both_light_deep_combine",
    "sleep_stages_numerical_map": {
        "Sleep stage 1": 2,
        "Sleep stage 2": 2,
        "Sleep stage 3": 1,
        "Sleep stage 4": 1,
        "Sleep stage ?": 0,
        "Sleep stage R": 3,
        "Sleep stage W": 4
    },
    "time_to_export": "248.29s",
    "total_clips": 59317,
    "type": "EDF"
}

Sleep stages count in training data (57360): [0, 7217, 33598, 10162, 6383] ([0.0, 0.1258, 0.5857, 0.1772, 0.1113])
Sleep stages count in validation set input (1952): [0, 436, 1042, 415, 59] ([0.0, 0.2234, 0.5338, 0.2126, 0.0302])
Sleep stages count in validation set prediction (1952, tf): [[0, 391, 1051, 474, 36]] ([0.0, 0.2003, 0.5384, 0.2428, 0.0184])
Sleep stages count in validation set prediction (1952, tflite): [[0, 391, 1051, 474, 36]] ([0.0, 0.2003, 0.5384, 0.2428, 0.0184])
Sleep stages count in validation set prediction (1952, tflite (quant)): [[0, 391, 1053, 470, 38]] ([0.0, 0.2003, 0.5394, 0.2408, 0.0195])
Sleep stages count in validation set prediction (1952, tflite (full quant)): [[0, 763, 924, 59, 206]] ([0.0, 0.3909, 0.4734, 0.0302, 0.1055])
Sleep stages count in validation set prediction (1952, tflite (16bx8b full quant)): [[0, 0, 0, 0, 0]] ([0.0, 0.0, 0.0, 0.0, 0.0])

Clip length (s): 30.0
Patch length (# of samples): 64
Number of sleep stages (includes unknown): 5
Data type: <dtype: 'float32'>
Batch size: 16
Embedding depth: 64
MHA number of heads: 8
Number of layers: 1
MLP dimension: 32
Number of dense (+ dropout) layers in MLP head before softmax: 1
Dropout rate: 0.300
Input argument of encoder's 2nd residual sum connection: inputs
Historical prediction lookback DNN depth: -1
Activation function of first dense layer in MLP layer and MLP head: swish
Class training weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}
Initial learning rate: 0.001000
Optimizer: Adam
Rescale layer enabled: True
Use classification token: True
Positional embedding enabled: True
Output filter type: pre_argmax
Number of samples in output filter: 3
Output filter self-reset threshold: -1
Model loss type: sparse_categorical_crossentropy
Note: k-fold sweep residual sum with inputs
